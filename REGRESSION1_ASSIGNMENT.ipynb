{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c46a1d-c381-4354-865b-b1650cac4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "1>Simple Linear Regression:\n",
    "    \n",
    "Simple Linear Regression is a statistical method used to model the relationship between two\n",
    "variables: a dependent variable (also known as the target or outcome variable) and an independent\n",
    "variable (also known as the predictor variable). The goal of simple linear regression is to find\n",
    "the best-fitting line (linear equation) that describes the linear relationship between the two \n",
    "variables. The equation of the line is given by:\n",
    "\n",
    "y=mx+b\n",
    "\n",
    "Multiple Linear Regression extends the concept of simple linear regression to include multiple \n",
    "independent variables. Instead of just one predictor variable, you now have \n",
    "n predictor variables. The goal is to model the relationship between the dependent variable and \n",
    "multiple independent variables using a linear equation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d50ba-58a7-4ca5-b7fa-5a7b89e263fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2Linear regression makes several assumptions to ensure the validity of the model and the accuracy\n",
    "of the results. Violation of these assumptions can lead to biased or unreliable results. The key \n",
    "assumptions of linear regression are as follows:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and the independent variables is assumed\n",
    "to be linear. You can check this assumption by plotting a scatter plot of the dependent variable against\n",
    "each independent variable separately and looking for a roughly linear pattern.\n",
    "\n",
    "Independence: The residuals (the differences between the actual and predicted values) should be independent \n",
    "of each other. This assumption ensures that there is no pattern or correlation left in the residuals. You \n",
    "can check for independence by plotting the residuals against the predicted values or against the order in \n",
    "which the data was collected. If there is no apparent pattern, the independence assumption is likely \n",
    "satisfied.c\n",
    "\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can take the following steps:\n",
    "\n",
    "Visual Inspection: Plot scatter plots of the dependent variable against each independent variable,\n",
    "as well as plots of the residuals against predicted values. These visualizations can provide insights\n",
    "into the linearity and homoscedasticity assumptions.\n",
    "\n",
    "Residual Analysis: Examine the residuals for patterns, such as heteroscedasticity or curvature. Additionally,\n",
    "create a histogram and a Q-Q plot of the residuals to assess normality.\n",
    "\n",
    "Correlation Analysis: Calculate correlation coefficients between independent variables to identify potential\n",
    "multicollinearity. Calculate VIF values to quantify multicollinearity.\n",
    "\n",
    "Domain Knowledge: Use your understanding of the subject matter to identify any potential endogeneity issues \n",
    "or other violations of assumptions that may not be easily detectable through statistical tests alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1d78e-168c-4c4c-87bc-1dde7f335ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Interpretations:\n",
    "\n",
    " In this scenario, the intercept doesn't have a meaningful interpretation since it implies a house with \n",
    "zero square footage, which is not realistic. It's included to provide the baseline reference point for\n",
    "the model but is usually not directly interpretable.\n",
    "\n",
    "Slope\n",
    ": The slope is the key parameter to interpret. For example, if \n",
    "\n",
    " is 100, it means that, on average, for every additional square foot of house area, the house price is \n",
    "    expected to increase by $100. Conversely, if  -50, it means that, on average, for every additional square foot of house area, the house price is expected to decrease by $50.\n",
    "\n",
    "Example:\n",
    "Let's say we fit the linear regression model to actual housing data, and we find that the estimated slope (\n",
    "\n",
    "  is 75. This means that, on average, for every additional square foot of house area, the house price is\n",
    "    expected to increase by $75. If a house has 200 square feet more than another house, we would expect\n",
    "    it to be priced approximately $75 * 200 = $15,000 higher, assuming all other factors remain constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62088c2f-0308-40cf-b4ec-790d304c8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Gradient Descent is an optimization technique used to find the minimum (or maximum) of a function,\n",
    "typically in the context of machine learning when training models. It's especially useful for finding\n",
    "the optimal parameters (weights and biases) of a model that minimize a cost function, leading to better\n",
    "model performance.\n",
    "\n",
    "In the context of machine learning, gradient descent helps us adjust the parameters of a model iteratively\n",
    "to minimize the difference between predicted and actual outcomes, which is captured by the cost function.\n",
    "The core idea is to follow the direction of the steepest decrease (negative gradient) of the cost function\n",
    "to reach a local minimum\n",
    "\n",
    "Batch Gradient Descent: In each iteration, the algorithm computes the gradient using the entire dataset.\n",
    "This method can be slow for large datasets but tends to provide a more accurate estimate of the gradient.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): In each iteration, the algorithm uses only one randomly selected data point\n",
    "to compute the gradient. SGD can be faster, but the updates can be noisy and might lead to more oscillations \n",
    "in the optimization path.\n",
    "\n",
    "Mini-Batch Gradient Descent: This is a compromise between batch and SGD. It uses a small random subset \n",
    "(mini-batch) of the data to compute the gradient. This approach combines the benefits of both batch and \n",
    "SGD, often leading to faster convergence and less noisy updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f181b7b-9428-495d-b4de-348cb3e95d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "5> Multiple Linear Regression is an extension of simple linear regression that allows us to model the\n",
    "relationship between a dependent variable and multiple independent variables. In simple linear regression,\n",
    "we have one dependent variable and one independent variable, while in multiple linear regression, we have\n",
    "one dependent variable and two or more independent \n",
    "\n",
    "\n",
    "Differences between Multiple Linear Regression and Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "In simple linear regression, there is only one independent variable.\n",
    "In multiple linear regression, there are two or more independent variables.\n",
    "Model Equation:\n",
    "\n",
    "In simple linear regression, the model equation has the form \n",
    "In multiple linear regression, the model equation includes multiple independent variables: \n",
    "\n",
    "\n",
    "\n",
    "Multiple linear regression models can capture more complex relationships by considering multiple predictors.\n",
    "Interpretation can be more intricate in multiple linear regression due to the interplay between multiple \n",
    "independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362e432-4c20-4b6a-a868-5b66abcaa8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "6> Multicollinearity is a phenomenon that occurs in multiple linear regression when two or more independent\n",
    "variables are highly correlated with each other. This can cause issues in the regression model, leading to\n",
    "unreliable coefficient estimates and difficulty in interpreting the relationships between the variables.\n",
    "\n",
    "When multicollinearity is present, it becomes challenging to distinguish the individual effects of each \n",
    "correlated variable on the dependent variable, as their effects tend to get mixed together. This can result \n",
    "in unstable and inflated coefficient estimates, making it harder to understand the true contributions of the\n",
    "variables in the model.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "There are several ways to detect multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables.\n",
    "High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): VIF quantifies how much the variance of the estimated regression coefficient\n",
    "is inflated due to multicollinearity. Higher VIF values (typically above 5 or 10) suggest multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove or Combine Variables: If two or more variables are highly correlated, consider removing one of them or\n",
    "creating a composite variable that combines their information.\n",
    "\n",
    "Regularization Techniques: Techniques like Ridge Regression and Lasso Regression add a penalty to the coefficients,\n",
    "encouraging the model to shrink some coefficients towards zero. This can help mitigate the impact of multicollinearity.\n",
    "\n",
    "Feature Selection: Choose a subset of the most relevant variables and exclude the highly correlated ones.\n",
    "\n",
    "Data Collection: Collect more data to reduce the impact of multicollinearity. A larger dataset can help alleviate\n",
    "the issue.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA transforms the original correlated variables into a new set of uncorrelated\n",
    "variables (principal components), which can help remove multicollinearity.\n",
    "\n",
    "Domain Knowledge: Use your understanding of the domain to decide which variables to include and exclude. Sometimes,\n",
    "correlated variables might make sense to keep if they provide distinct information.\n",
    "\n",
    "Model Comparison: Compare models with and without certain variables to see if their inclusion significantly affects \n",
    "the results. If not, you might decide to exclude some variables to address multicollinearity.\n",
    "\n",
    "Multicollinearity-Tolerant Algorithms: Some machine learning algorithms, like tree-based models (Random Forest,\n",
    "Gradient Boosting), are less affected by multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23831d09-d781-44c7-9749-9fdabf52cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "7>Polynomial Regression is a type of regression analysis that extends the concept of linear regression\n",
    "by introducing polynomial terms (terms raised to powers greater than 1) to model nonlinear relationships \n",
    "between the dependent variable and the independent variable(s). While linear regression assumes a linear \n",
    "relationship, polynomial regression allows for more flexible modeling of curvilinear patterns.\n",
    "\n",
    "Differences Between Polynomial Regression and Linear Regression:\n",
    "\n",
    "Model Equation:\n",
    "\n",
    "In linear regression, the model equation is a linear combination of the independent variables.\n",
    "In polynomial regression, the model equation includes polynomial terms (squared, cubed, etc.) of the independent\n",
    "variable(s), allowing for curvature in the relationship.\n",
    "Linearity:\n",
    "\n",
    "Linear regression assumes a linear relationship between the variables, which may not accurately capture complex, \n",
    "nonlinear relationships.\n",
    "Polynomial regression can capture nonlinear patterns by introducing polynomial terms, providing a more flexible\n",
    "modeling approach.\n",
    "Degree of Flexibility:\n",
    "\n",
    "Linear regression is less flexible and may not fit well to data with nonlinear patterns.\n",
    "Polynomial regression can offer greater flexibility by allowing the model to fit curves, bends, and more intricate\n",
    "patterns.\n",
    "Overfitting:\n",
    "\n",
    "Linear regression tends to have lower risk of overfitting, as it assumes a simpler relationship.\n",
    "Polynomial regression with higher degrees of polynomial terms can lead to overfitting if not properly regularized.\n",
    "Interpretability:\n",
    "\n",
    "Linear regression models are generally more interpretable due to their simpler structure.\n",
    "Polynomial regression models with higher degrees become more complex and may be harder to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86572d1-44f8-4180-bb3c-13de66e244cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "8 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
